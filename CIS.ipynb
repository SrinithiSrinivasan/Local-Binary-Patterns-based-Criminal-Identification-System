{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIS.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SrinithiSrinivasan/Local-Binary-Patterns-based-Criminal-Identification-System/blob/master/CIS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKAfGa2Ana6l",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Detection Module\n",
        "def detect_face(img):\n",
        "    \n",
        "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "    face_cascade = cv2.CascadeClassifier('opencv-files/lbpcascade_frontalface.xml')\n",
        "    faces = face_cascade.detectMultiScale(gray, scaleFactor=1.2, minNeighbors=5);\n",
        "    if (len(faces) == 0):\n",
        "        return None, None\n",
        "    (x, y, w, h) = faces[0]\n",
        "    return gray[y:y+w, x:x+h], faces[0]\n",
        "\n",
        "#Preparation Module\n",
        "def prepare_training_data(data_folder_path):\n",
        "    \n",
        "    dirs = os.listdir(data_folder_path)\n",
        "    faces = []\n",
        "    labels = []\n",
        "    for dir_name in dirs:\n",
        "        if not dir_name.startswith(\"s\"):\n",
        "            continue;\n",
        "        \n",
        "        label = int(dir_name.replace(\"s\", \"\"))\n",
        "        subject_dir_path = data_folder_path + \"/\" + dir_name\n",
        "        subject_images_names = os.listdir(subject_dir_path)\n",
        "        for image_name in subject_images_names:\n",
        "            if image_name.startswith(\".\"):\n",
        "                continue;\n",
        "            image_path = subject_dir_path + \"/\" + image_name\n",
        "           image = cv2.imread(image_path)\n",
        "            #cv2.imshow(\"Training on image...\", cv2.resize(image, (400, 500)))\n",
        "            cv2.resize(image, (400, 500))\n",
        "            #cv2.waitKey(100)\n",
        "            face, rect = detect_face(image)\n",
        "            if face is not None:\n",
        "                faces.append(face)\n",
        "                labels.append(label)\n",
        "            \n",
        "    cv2.destroyAllWindows()\n",
        "    cv2.waitKey(1)\n",
        "    cv2.destroyAllWindows()\n",
        "    \n",
        "    return faces, labels\n",
        "\n",
        "print(\"Preparing data...\")\n",
        "faces, labels = prepare_training_data(\"training-data\")\n",
        "print(\"Data prepared\")\n",
        "print(\"Total faces: \", len(faces))\n",
        "print(\"Total labels: \", len(labels))\n",
        "\n",
        "#Recognition and Prediction Module\n",
        "def predict(test_img):\n",
        "    img = test_img.copy()\n",
        "    t1 = time.time()\n",
        "    face, rect = detect_face(img)\n",
        "    t2 = time.time()\n",
        "    dt1 = t2 - t1\n",
        "    print(\"Time taken for detection:\")\n",
        "    print(dt1)\n",
        "    t3=time.time()\n",
        "    label, confidence = face_recognizer.predict(face)\n",
        "    val=face_recognizer.getHistograms()\n",
        "    print(\"Confidence:\")\n",
        "    print(confidence)\n",
        "    t4=time.time()\n",
        "    dt2=t4-t2\n",
        "    print(\"Time taken for prediction:\")\n",
        "    print(dt2)\n",
        "    \n",
        "    text=subjects[label]\n",
        "    label_text = text\n",
        "    draw_rectangle(img, rect)\n",
        "    draw_text(img, label_text, rect[0], rect[1]-5)\n",
        "    return img,text\n",
        "\n",
        "#Summary Module\n",
        "def read_article(file_name):\n",
        "    file = open(file_name, \"r\")\n",
        "    filedata = file.readlines()\n",
        "    article = filedata[0].split(\". \")\n",
        "    sentences = []\n",
        "   for sentence in article:\n",
        "        print(sentence)\n",
        "        print('\\n')\n",
        "        sentences.append(sentence.replace(\"[^a-zA-Z]\", \" \").split(\" \"))\n",
        "    sentences.pop() \n",
        "    return sentences\n",
        "\n",
        "def sentence_similarity(sent1, sent2, stopwords=None):\n",
        "    if stopwords is None:\n",
        "        stopwords = []\n",
        " sent1 = [w.lower() for w in sent1]\n",
        "    sent2 = [w.lower() for w in sent2]\n",
        "    all_words = list(set(sent1 + sent2))\n",
        "    vector1 = [0] * len(all_words)\n",
        "    vector2 = [0] * len(all_words)\n",
        "    for w in sent1:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector1[all_words.index(w)] += 1     \n",
        "    for w in sent2:\n",
        "        if w in stopwords:\n",
        "            continue\n",
        "        vector2[all_words.index(w)] += 1 \n",
        "    return 1 - cosine_distance(vector1, vector2) \n",
        "def build_similarity_matrix(sentences, stop_words):\n",
        "    # Create an empty similarity matrix\n",
        "    similarity_matrix = np.zeros((len(sentences), len(sentences))) \n",
        "    for idx1 in range(len(sentences)):\n",
        "        for idx2 in range(len(sentences)):\n",
        "            if idx1 == idx2: #ignore if both are same sentences\n",
        "                Continue\n",
        "similarity_matrix[idx1][idx2] = sentence_similarity(sentences[idx1], sentences[idx2], stop_words)\n",
        "     return similarity_matrix\n",
        "def generate_summary(file_name, top_n=3):\n",
        "     stop_words = stopwords.words('english')\n",
        "    summarize_text = []\n",
        "    sentences =  read_article(file_name)\n",
        "    sentence_similarity_martix = build_similarity_matrix(sentences, stop_words)x\n",
        "    sentence_similarity_graph = nx.from_numpy_array(sentence_similarity_martix)\n",
        "    scores = nx.pagerank(sentence_similarity_graph)\n",
        "    ranked_sentence = sorted(((scores[i],s) for i,s in enumerate(sentences)), reverse=True)    \n",
        "    print(\"Indexes of top ranked_sentence order are \", ranked_sentence)    \n",
        "    for i in range(top_n):\n",
        "      summarize_text.append(\" \".join(ranked_sentence[i][1]))\n",
        " \n",
        "    str=\"\"\n",
        "    str1=str.join(summarize_text)\n",
        "    print(summarize_text)\n",
        "    f1=open(\"otp.txt\",\"w\")\n",
        "    f1.write(str1)\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}